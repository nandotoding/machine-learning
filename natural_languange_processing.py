# -*- coding: utf-8 -*-
"""Natural Languange Processing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1StHMVVwMXG5zqsACACMDpqhupLP5AKOY

# Natural Languange Processing
# Multiclass Text Classification
BBC Text Classification

# Dataset Preparation
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import nltk

df = pd.read_csv(r'/content/drive/MyDrive/Datasets/bbc-text.csv')
df.head()

df.shape

df.category.value_counts()

df.text[0]

"""# Stop words"""

nltk.download('stopwords')

from nltk.corpus import stopwords

sw_list = stopwords.words("english")
df.text = df.text.apply(lambda x: [item for item in x.split() if item not in sw_list]).apply(lambda x:" ".join(x))
df.text[0]

"""# Lemmatization"""

nltk.download('wordnet')
nltk.download('omw-1.4')

from nltk.stem import WordNetLemmatizer
from nltk.tokenize import WhitespaceTokenizer

lemmatizer = WordNetLemmatizer()
w_tokenizer = nltk.tokenize.WhitespaceTokenizer()

def lemmatize_text(text):
    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]

df.text = df.text.apply(lemmatize_text)

df.text[:5]

"""# One Hot Encode"""

dfLabels = pd.get_dummies(df.category)
dfNew = pd.concat([df, dfLabels], axis=1)
dfNew = dfNew.drop(labels=['category'], axis=1)
dfNew.head(3)

"""# Train-Test Split"""

X = dfNew.values[:, 0]
y = dfNew.values[:, 1:]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)

"""# Tokenization"""

tokenizer = Tokenizer(num_words=50000, oov_token='x', lower=True)

tokenizer.fit_on_texts(X_train)
tokenizer.fit_on_texts(X_test)
XTrainSeq = tokenizer.texts_to_sequences(X_train)
XTestSeq = tokenizer.texts_to_sequences(X_test)
XTrainPad = pad_sequences(XTrainSeq)
XTestPad = pad_sequences(XTestSeq)

y_train = np.asarray(y_train).astype(np.float32)
y_test = np.asarray(y_test).astype(np.float32)

"""# Model Training and Validation"""

model = tf.keras.models.Sequential([
    tf.keras.layers.Embedding(50000, 64),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, recurrent_regularizer='l2')),
    tf.keras.layers.Dropout(0.25),
    tf.keras.layers.Dense(units=64, activation='relu'),
    tf.keras.layers.Dropout(0.25),
    tf.keras.layers.Dense(units=5, activation='softmax')
])

model.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if((logs.get('accuracy') > 0.9) and (logs.get('val_accuracy') > 0.9)):
      self.model.stop_training = True
      print("\nAccuracy has exceeded 90%!")

callbacks = myCallback()

hist = model.fit(XTrainPad, y_train, epochs=30, validation_data=(XTestPad, y_test), verbose=2, callbacks=[callbacks])

"""# Accuracy and Loss Plot"""

plt.plot(hist.history['accuracy'])
plt.plot(hist.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.tight_layout()
plt.show()

plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.tight_layout()
plt.show()